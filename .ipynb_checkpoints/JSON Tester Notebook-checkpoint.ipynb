{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This noteboook is designed as a prototype script for grabbing articles from the JSON file.\n",
    "\n",
    "First let's create a small batch of the JSON objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pathway = \"/Volumes/Seagate Backup Plus Drive 1/RC_2015-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import shelve\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.append('[deleted]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', '[deleted]']\n"
     ]
    }
   ],
   "source": [
    "print stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def json_to_db(db_name,num_of_comments):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Goes through the txt file, grabs the JSON objects in it.\n",
    "    Uses shelve to create a database, with key= Subreddit and continually appending text to value.\n",
    "    '''\n",
    "    # open database\n",
    "    d = shelve.open(db_name)\n",
    "    \n",
    "    # Set counter\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate through N JSON objects in file\n",
    "    with open(pathway) as myfile:\n",
    "        for item in myfile:\n",
    "            if count < num_of_comments:\n",
    "                \n",
    "                # Load the JSON object\n",
    "                json_object = json.loads(item)\n",
    "                \n",
    "                # Now append additional text to database\n",
    "                if d.has_key(json_object['subreddit']):\n",
    "                    \n",
    "                    temp = d[json_object['subreddit']]      # extracts the copy\n",
    "                    temp = temp+' '+json_object['body']     # mutates the copy\n",
    "                    d[json_object['subreddit']] = temp      # stores the copy right back, to persist it\n",
    "\n",
    "                # Or set the first entry for that subreddit\n",
    "                else:\n",
    "                    d[json_object['subreddit']] = json_object['body']\n",
    "                \n",
    "    \n",
    "                count += 1\n",
    "            else:\n",
    "                d.close()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_to_db('10000_comments',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = shelve.open('10000_comments.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Thanks! Now on to NYE makeup! [deleted] Right!? My face loves me, my wallet hates me.  Simple and Tasteful. :) Dipbrow is definitely something that has to be learned haha. I just barely stick the end of my brush into the pot and thats usually enough to cover my whole brow. But I also use the brow wiz first to outline my brows.  They look great on you, especially the red. How much do these cost if you don't mind me asking? Yeah she did that in high school...I've also recently read that she lied about being molested. Not sure how I feel about her now especially since she hardly does beauty vids anymore. She def is lacking when it comes to effort in her vids. I mixed a little of a Tarte stain (berry colored) with the baby lips clear chapstick, I've had the Tarte stain forever and the label is worn off so i dont know the exact color but i'll try and find out! and thank you!! Seriously. The recommendations for Gerard cosmetics need to chill the fuck out. Sona Gasparian's no makeup-makeup tutorial is the best one I've ever seen. Her YouTube username is makeupbysona.  You look awesome! I wish my eyebrows were that nice \\U0001f61e OH! well they're great and have a ton of eyelashes for super cheap I believe these ones are [jo jo](http://ohmy-lash.com/collections/new-sweetheart-collectiion/products/jo-jo) from the sweetheart collection, and it looks like they took down the subscription only option so you can \\nbuy them individually again! It doesn't take much water, I get my fingers wet with hot water and gently press my fingers together over my lashes.  No pulling, just touching them together around the lashes.  Agreed. Beauty gurus are lacking beauty vids and make up for it with hauls, q&amp;a's, sponsored vids, and vlogs. Those are cool every once in awhile but I watch beauty vids for help with beauty techniques, tutorials, and reviews on products I'm interested in. oh no! I love her, I hope it's just rumors :( Thanks! Now on to NYE makeup! [deleted] Right!? My face loves me, my wallet hates me.  Simple and Tasteful. :) Dipbrow is definitely something that has to be learned haha. I just barely stick the end of my brush into the pot and thats usually enough to cover my whole brow. But I also use the brow wiz first to outline my brows.  They look great on you, especially the red. How much do these cost if you don't mind me asking? Yeah she did that in high school...I've also recently read that she lied about being molested. Not sure how I feel about her now especially since she hardly does beauty vids anymore. She def is lacking when it comes to effort in her vids. I mixed a little of a Tarte stain (berry colored) with the baby lips clear chapstick, I've had the Tarte stain forever and the label is worn off so i dont know the exact color but i'll try and find out! and thank you!! Seriously. The recommendations for Gerard cosmetics need to chill the fuck out. Sona Gasparian's no makeup-makeup tutorial is the best one I've ever seen. Her YouTube username is makeupbysona.  You look awesome! I wish my eyebrows were that nice \\U0001f61e OH! well they're great and have a ton of eyelashes for super cheap I believe these ones are [jo jo](http://ohmy-lash.com/collections/new-sweetheart-collectiion/products/jo-jo) from the sweetheart collection, and it looks like they took down the subscription only option so you can \\nbuy them individually again! It doesn't take much water, I get my fingers wet with hot water and gently press my fingers together over my lashes.  No pulling, just touching them together around the lashes.  Agreed. Beauty gurus are lacking beauty vids and make up for it with hauls, q&amp;a's, sponsored vids, and vlogs. Those are cool every once in awhile but I watch beauty vids for help with beauty techniques, tutorials, and reviews on products I'm interested in. oh no! I love her, I hope it's just rumors :(\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['MakeupAddiction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Cleaning Text\n",
    "\n",
    "Here we focus on converting the large string into a list of words (tokenized and stopwords removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Output tokenizes text and removes any stopwords and then outptus lowercased words\n",
    "output = [word.lower() for word in tokenizer.tokenize(d['MakeupAddiction']) if not word.lower() in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "722"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(d['MakeupAddiction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final word cleaner code is below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def text_cleaner(text):\n",
    "    '''\n",
    "    INPUT: string of body text\n",
    "    OUTPUT: List of tokenized lower case words with stopwords removed\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Output tokenizes text and removes any stopwords and then outptus lowercased words\n",
    "    return [word.lower() for word in tokenizer.tokenize(text) if not word.lower() in stopwords]\n",
    "    \n",
    "#text_cleaner(d['MakeupAddiction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONGO DB Setup\n",
    "\n",
    "This focuses on setting up a MongoDB and Push the new body of comments to the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'subreddit3': [u'text1', u'text2'], u'askreddit': [u'what', u'who', u'why'], u'greetings': [u'hi', u'hello'], u'string2': 2, u'string3': 3, u'string1': 34}\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "# Connect with db\n",
    "reddit_data = pymongo.Connection()['reddit_database']['comments']\n",
    "\n",
    "def update_mongo(subreddit,comment):\n",
    "    reddit_data.update({'subreddit':subreddit},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': ObjectId('55c1b21e684059792f5820c7'), u'title': u'test'}\n"
     ]
    }
   ],
   "source": [
    "# make a connection\n",
    "import pymongo\n",
    "articles = pymongo.Connection()['test']['articles']\n",
    "\n",
    "# insert an article and print out it's contents (make sure it's there)\n",
    "articles.insert({\"title\": \"tyler's article\"})\n",
    "article = articles.find_one()\n",
    "#article\n",
    "#{u'_id': ObjectId('4f0b353c096f762312000002'), u'title': u\"tyler's article\"}\n",
    "\n",
    "# update the title to something new\n",
    "article['title'] = \"test\"\n",
    "\n",
    "# save it\n",
    "articles.save(article)\n",
    "\n",
    "# check to see if it changed\n",
    "print articles.find_one()\n",
    "#{u'_id': ObjectId('4f0b353c096f762312000002'), u'title': u'test'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'n': 1, u'nModified': 1, u'ok': 1, 'updatedExisting': True}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# update the email address and the score at the same time \n",
    "# using $set in a single write. \n",
    "collection.update({\"_id\":\"subreddit3\"},\n",
    "{\"$push\":{\"value\":['this','is','even','more','stuff']}}, safe=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "newD = {}\n",
    "for obj in collection.find():\n",
    "    newD[obj['_id']] = obj['value']\n",
    "print newD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "update() takes at least 3 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-dbb7f0b22915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"$push\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'string2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: update() takes at least 3 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "#### EXAMPLE\n",
    "\n",
    "#cleaner will tokenize and clean long string\n",
    "def cleaner(text):\n",
    "    pass\n",
    "\n",
    "for word in cleaner(json_object['body']):\n",
    "    collection.update({'subreddit': json_object['subreddit']},\n",
    "                      {'$push':{'body':word}},\n",
    "                      safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import Connection\n",
    "\n",
    "# Example dictionary\n",
    "d = {'greetings' : ['hi','hello'], 'askreddit' : ['what','who','why'], 'subreddit3' : ['text1','text2']}\n",
    "\n",
    "\n",
    "#conn = Connection()\n",
    "#db = conn['example-database']\n",
    "#collection = db['example-collection']\n",
    "\n",
    "for sub, body in d.items():\n",
    "    collection.save({'_id' : sub, 'value' : body})\n",
    "# testing\n",
    "newD = {}\n",
    "for obj in collection.find():\n",
    "    newD[obj['_id']] = obj['value']\n",
    "print newD\n",
    "# output is: {u'string2': 2, u'string3': 3, u'string1': 1}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
