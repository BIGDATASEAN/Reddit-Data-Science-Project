{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This noteboook is designed as a prototype script for grabbing articles from the JSON file.\n",
    "\n",
    "First let's create a small batch of the JSON objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pathway = \"/Volumes/Seagate Backup Plus Drive 1/RC_2015-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import shelve\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.append('[deleted]')\n",
    "#stopwords.append('deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', '[deleted]', 'deleted']\n"
     ]
    }
   ],
   "source": [
    "print stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to quickly make a python shelve database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def json_to_db(db_name,num_of_comments):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Goes through the txt file, grabs the JSON objects in it.\n",
    "    Uses shelve to create a database, with key= Subreddit and continually appending text to value.\n",
    "    '''\n",
    "    # open database\n",
    "    d = shelve.open(db_name)\n",
    "    \n",
    "    # Set counter\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate through N JSON objects in file\n",
    "    with open(pathway) as myfile:\n",
    "        for item in myfile:\n",
    "            if count < num_of_comments:\n",
    "                \n",
    "                # Load the JSON object\n",
    "                json_object = json.loads(item)\n",
    "                \n",
    "                # Now append additional text to database\n",
    "                if d.has_key(json_object['subreddit']):\n",
    "                    \n",
    "                    temp = d[json_object['subreddit']]      # extracts the copy\n",
    "                    temp = temp+' '+json_object['body']     # mutates the copy\n",
    "                    d[json_object['subreddit']] = temp      # stores the copy right back, to persist it\n",
    "\n",
    "                # Or set the first entry for that subreddit\n",
    "                else:\n",
    "                    d[json_object['subreddit']] = json_object['body']\n",
    "                \n",
    "    \n",
    "                count += 1\n",
    "            else:\n",
    "                d.close()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_to_db('10000_comments',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = shelve.open('10000_comments.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"This is a great question, and I want to thank you for asking it. However, I don't have any answers. I'm interested in learning more myself. \""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['needadvice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Cleaning Text\n",
    "\n",
    "Here we focus on converting the large string into a list of words (tokenized and stopwords removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Output tokenizes text and removes any stopwords and then outptus lowercased words\n",
    "output = [word.lower() for word in tokenizer.tokenize(d['needadvice']) if not word.lower() in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(d['needadvice']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final word cleaner code is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.append('[deleted]')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def text_cleaner(text):\n",
    "    '''\n",
    "    INPUT: string of body text\n",
    "    OUTPUT: List of tokenized lower case words with stopwords removed\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Output tokenizes text and removes any stopwords and then outptus lowercased words\n",
    "    return [word.lower() for word in tokenizer.tokenize(text) if not word.lower() in stopwords]\n",
    "    \n",
    "#text_cleaner(d['MakeupAddiction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONGO DB Setup\n",
    "\n",
    "This focuses on setting up a MongoDB and Push the new body of comments to the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# Connect with db\n",
    "#reddit_data = pymongo.Connection()['reddit_database']['comments']\n",
    "\n",
    "def update_mongo(collection,subreddit,comment):\n",
    "    '''\n",
    "    INPUT: subreddit name and comment as a cleaned list of words\n",
    "    RESULT: Updates MongoDB reddit_data database with $push\n",
    "    '''\n",
    "    \n",
    "    # NOTE: This pushes the entire list, so final result is a list of comment lists\n",
    "    # Use itertools.chain.from_iterable(reddit_data[subreddit]) to combine this for doc2vec\n",
    "    collection.update({'subreddit': subreddit},\n",
    "                      {'$push':{'body':comment}},upsert = True,\n",
    "                      safe=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_test = text_cleaner(d['MakeupAddiction'])\n",
    "update_mongo(reddit_data,'MakeupAddiction',comment_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clears database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clear_mongo(mongodb):\n",
    "    '''\n",
    "    INPUT: A mongodb database\n",
    "    RESULT: Based on user input, clears database.\n",
    "    '''\n",
    "    ans = raw_input('WARNING: THIS WILL CLEAR THE DATABASE. ARE YOU SURE? y/n')\n",
    "    if ans == 'y':\n",
    "        mongodb.remove()\n",
    "        print 'Database has been cleared.'\n",
    "    else:\n",
    "        print 'Database not cleared.'\n",
    "        \n",
    "        \n",
    "#clear_mongo(reddit_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take in SubReddit, return document list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "def get_doc_list(db_collection,subreddit):\n",
    "    '''\n",
    "    INPUT: Subreddit name\n",
    "    OUTPUT: A document = list of words for that subreddit\n",
    "    '''\n",
    "    \n",
    "    for obj in db_collection.find({'subreddit':subreddit}):\n",
    "        return list(itertools.chain.from_iterable(obj['body']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get_doc_list(reddit_data,'AskReddit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a MongoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "def create_or_connect(database,collection):\n",
    "    '''\n",
    "    INPUT: Database name (str) and collection (name)\n",
    "    OUTPUT: Either creates the database or connects to it if it already exists.\n",
    "            Returns the collections in the database cursor.\n",
    "    '''\n",
    "    \n",
    "    return pymongo.Connection()[database][collection]\n",
    "\n",
    "#create_or_connect('check','test_coll') works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MongoDB from my Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reddit_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-42e07fbb5f5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Clear the db first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclear_mongo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreddit_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reddit_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Clear the db first\n",
    "clear_mongo(reddit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make or Connect to Database\n",
    "reddit_data = create_or_connect('reddit_database','reddit_comments')\n",
    "\n",
    "def reddit_data_pusher(db_name,num_of_comments):\n",
    "    \n",
    "    '''\n",
    "    Goes through the txt file, grabs the JSON objects in it.\n",
    "    Uses MongoDB to create a database, with key= Subreddit and continually appending clean text word list to value.\n",
    "    '''\n",
    "    \n",
    "    # Set counter\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate through N JSON objects in file\n",
    "    with open(pathway) as myfile:\n",
    "        for item in myfile:\n",
    "            if count < num_of_comments:\n",
    "                \n",
    "                # Load the JSON object\n",
    "                json_object = json.loads(item)\n",
    "                \n",
    "                # Clean and tokenize text\n",
    "                body = text_cleaner(json_object['body'])\n",
    "                \n",
    "                # $push to MongoDB\n",
    "                update_mongo(reddit_data,json_object['subreddit'],body)\n",
    "                \n",
    "                # For safety puposes\n",
    "                count += 1\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a MongoDB with N comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clear_mongo(reddit_data)\n",
    "reddit_data = create_or_connect('reddit_database','reddit_comments')\n",
    "reddit_data_pusher(reddit_data,500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Doc2Vec Model Training\n",
    "\n",
    "Let's try to train a model on a very small corpus (10,000 comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_gen(database):\n",
    "    for item in database.find():\n",
    "        words=list(itertools.chain.from_iterable(item['body']))\n",
    "        yield LabeledSentence(words,labels=[str(item['subreddit'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "import gensim.models.doc2vec\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "\n",
    "# Doc2Vec(dbow,d100,n5,mc2,t8)\n",
    "d2v_reddit_model = Doc2Vec( dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores)\n",
    "d2v_reddit_model.build_vocab(sentence_gen(reddit_data))\n",
    "#d2v_reddit_model.train(sentence_gen(reddit_data))\n",
    "\n",
    "for epoch in range(10):\n",
    "    d2v_reddit_model.train(sentence_gen(reddit_data))\n",
    "    d2v_reddit_model.alpha -= 0.002  # decrease the learning rate\n",
    "    d2v_reddit_model.min_alpha = d2v_reddit_model.alpha  # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d2v_reddit_model.save('small_reddit_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Generator of Labeled Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "    d2v_reddit_model.train(sentence_gen(reddit_data))\n",
    "    d2v_reddit_model.alpha -= 0.002  # decrease the learning rate\n",
    "    d2v_reddit_model.min_alpha = d2v_reddit_model.alpha  # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qq = LabeledSentence(words=['alpha','beta','charlie'],labels=['first'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nfor epoch in range(10):\\n    model.train(sentence_gen(reddit_data))\\n    model.alpha -= 0.002  # decrease the learning rate\\n    model.min_alpha = model.alpha  # fix the learning rate, no decay\\n'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Doc2Vec(alpha=0.025, min_alpha=0.025)  # use fixed learning rate\n",
    "model.build_vocab(sentence_gen(reddit_data))\n",
    "model.train(sentence_gen(reddit_data))\n",
    "''' \n",
    "for epoch in range(10):\n",
    "    model.train(sentence_gen(reddit_data))\n",
    "    model.alpha -= 0.002  # decrease the learning rate\n",
    "    model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_load = Doc2Vec.load('my_first_reddit_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'disputed', 0.38245826959609985),\n",
       " (u'technological', 0.36328911781311035),\n",
       " (u'interns', 0.35996055603027344),\n",
       " (u'seizures', 0.3545945882797241),\n",
       " (u'skimpy', 0.35189908742904663),\n",
       " (u'1071', 0.3478209376335144),\n",
       " (u'tangles', 0.34697842597961426),\n",
       " (u'622', 0.346447229385376),\n",
       " (u'whammy', 0.3443041443824768),\n",
       " (u'lawful', 0.3420853316783905)]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_load.most_similar(positive=['politics'],negative=['guns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Offensive_Wallpapers', 0.8445112705230713),\n",
       " ('Waxpen', 0.8392848372459412),\n",
       " ('FlossCrew', 0.8378405570983887),\n",
       " ('HereComesTheBoom', 0.8365437984466553),\n",
       " ('SEO', 0.8363415002822876),\n",
       " ('Blondeass', 0.8363173604011536),\n",
       " ('Starmade', 0.8361822366714478),\n",
       " ('aal', 0.8359136581420898),\n",
       " ('bulletjournal', 0.8358200788497925),\n",
       " ('Iowa', 0.8352646231651306)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_reddit_model.most_similar('AskReddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guncontrol', 0.8933992981910706),\n",
       " ('TankPorn', 0.8930748105049133),\n",
       " ('joerogan2', 0.8922608494758606),\n",
       " ('omnisregion', 0.8921858072280884),\n",
       " ('GearsOfWar', 0.8897333145141602),\n",
       " ('ajlee', 0.889672040939331),\n",
       " ('GirlsPlayingSports', 0.8894907832145691),\n",
       " ('sex_comics', 0.8894057273864746),\n",
       " ('lgg2', 0.8887702822685242),\n",
       " ('japancirclejerk', 0.8885037302970886)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_reddit_model.most_similar('guns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guncontrol', 0.9355584979057312),\n",
       " ('TankPorn', 0.9350743889808655),\n",
       " ('GirlsPlayingSports', 0.9337582588195801),\n",
       " ('Innie', 0.9314041137695312),\n",
       " ('howyoudoin', 0.9310590624809265),\n",
       " ('AnimalsBeingBros', 0.9301647543907166),\n",
       " ('shittynosleep', 0.9299746155738831),\n",
       " ('FIU', 0.9298853874206543),\n",
       " ('pornID', 0.9298760890960693),\n",
       " ('VirginiaTech', 0.9296820163726807)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_reddit_model.most_similar('guns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dogtraining', 0.9423602819442749),\n",
       " ('BlackMetal', 0.9338551759719849),\n",
       " ('idm', 0.9338396787643433),\n",
       " ('sugargliders', 0.9335150122642517),\n",
       " ('Bulges', 0.9334157705307007),\n",
       " ('NakedProgress', 0.932555615901947),\n",
       " ('zoophilia', 0.9324568510055542),\n",
       " ('batepapo', 0.932345986366272),\n",
       " ('GirlsinLaceFishnets', 0.9321831464767456),\n",
       " ('psn', 0.9320716857910156)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_reddit_model.most_similar('dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEED TO DO:\n",
    "\n",
    "PUT THIS INTO A CLEAN OOP PY FILE\n",
    "GET SPARK ROLLING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qq = LabeledSentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
